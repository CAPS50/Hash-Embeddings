{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-14T14:00:14.871142Z",
     "start_time": "2018-01-14T14:00:14.861467Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from evaluate.load.helpers import *\n",
    "from evaluate.load.dataset import *\n",
    "from evaluate.pipeline.model import *\n",
    "from evaluate.pipeline.trainer import *\n",
    "from evaluate.pipeline.embedding import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_param(mode=\"paper\",isHashEmbedding=True):\n",
    "    return {\"use_hash_embeddings\": isHashEmbedding,\n",
    "            \"hashing_trick\": True,\n",
    "            \"ngram_range\": (1,9),\n",
    "            \"nFeaturesRange\": (4,100),\n",
    "            \"embedding_size\": 20,\n",
    "            \"num_buckets\": 10**6,\n",
    "            \"max_features\": 10**7,\n",
    "            \"max_epochs\": 100,\n",
    "            \"num_hash_functions\": 2,\n",
    "            \"hidden\": 50,\n",
    "            \"seed\": 123,\n",
    "            \"batch_size\": 32,\n",
    "            \"masking\":True,\n",
    "            \"append_weight\":False,\n",
    "            \"validation_size\":0.05,\n",
    "            \"patience\":10,\n",
    "            'cuda':True,\n",
    "            'num_workers':4}\n",
    "            \n",
    "param = get_param()\n",
    "seed = param['seed']\n",
    "max_features = param['max_features']\n",
    "ngram_range = param['ngram_range']\n",
    "use_hash_embeddings = param['use_hash_embeddings']\n",
    "isHashingTrick = param['hashing_trick']\n",
    "num_buckets = param['num_buckets']\n",
    "embedding_size = param['embedding_size']\n",
    "max_epochs = param['max_epochs']\n",
    "masking = param['masking']\n",
    "append_weight = param['append_weight']\n",
    "patience = param['patience']\n",
    "num_hash_functions = param['num_hash_functions']\n",
    "batch_size = param['batch_size']\n",
    "validation_size = param['validation_size']\n",
    "nFeaturesRange = param['nFeaturesRange']\n",
    "isCuda = param['cuda']\n",
    "num_workers = param['num_workers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "isHashingTrick = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "v=Vocabulary(num_words=5,ngramRange=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.fit('hello my name is yes yann yes yann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5, 1, 2, 1, 2, 3, 3]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(v.tokenize('hello my name is yes yann yes yann'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v['yes yann']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 5, 6, 7, 8, 9, 10, 11, 12, 11]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(v.fit_tokenize('hello my name is yes yann yes yann'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'ngram_range'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Hash-Embeddings/evaluate/load/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, classes, path, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m                  \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"../data/ag_news_csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                  **kwargs):\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAgNews\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Hash-Embeddings/evaluate/load/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, train, isHashingTrick, nFeaturesRange, trainVocab, seed, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'ngram_range'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "if isHashingTrick:\n",
    "    train = AgNews(nFeaturesRange=nFeaturesRange,train=True,ngramRange=ngram_range,isHashingTrick=True,seed=seed)\n",
    "    test = AgNews(nFeaturesRange=nFeaturesRange,train=False,ngramRange=ngram_range,isHashingTrick=True,seed=seed)\n",
    "else:\n",
    "    train = AgNews(nFeaturesRange=nFeaturesRange,num_words=max_features,train=True,ngram_range=ngram_range,isHashingTrick=False,seed=seed)\n",
    "    test = AgNews(nFeaturesRange=nFeaturesRange,num_words=max_features,train=False,ngram_range=ngram_range,isHashingTrick=False,seed=seed,trainVocab=train.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train, valid = train_valid_load(train,validSize=0.1,isShuffle=True,seed=123,batch_size=batchSize)\n",
    "test = DataLoader(dataset=test,batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 2\n",
       " 2\n",
       " 2\n",
       "⋮ \n",
       " 1\n",
       " 1\n",
       " 1\n",
       "[torch.LongTensor of size 120000]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.labels = train.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1348])\n",
      "torch.Size([32])\n",
      "\n",
      "-5.9271e+18  1.4534e+18 -6.6569e+17  ...   0.0000e+00  0.0000e+00  0.0000e+00\n",
      " 1.3309e+18  6.5815e+18  4.9286e+18  ...   0.0000e+00  0.0000e+00  0.0000e+00\n",
      " 8.4722e+18  4.4392e+18 -3.4756e+18  ...   0.0000e+00  0.0000e+00  0.0000e+00\n",
      "                ...                   ⋱                   ...                \n",
      " 5.5544e+18 -6.2125e+18 -7.9083e+18  ...   0.0000e+00  0.0000e+00  0.0000e+00\n",
      "-3.4780e+18  8.9662e+18  3.9883e+18  ...   0.0000e+00  0.0000e+00  0.0000e+00\n",
      " 2.6035e+18  1.1896e+18  9.0944e+18  ...   0.0000e+00  0.0000e+00  0.0000e+00\n",
      "[torch.LongTensor of size 32x1348]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x,y in DataLoader(dataset=train,batch_size=batch_size,shuffle=True):\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.LongTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(train.classes)\n",
    "model = ModelNoDict(max_features,embedding_size,num_classes,isHash=True,append_weight=True,num_buckets=num_buckets)\n",
    "if isCuda:\n",
    "    model.cuda\n",
    "trainer = Trainer(model,seed=seed)\n",
    "callbacks = [EarlyStopping(patience=patience)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=Counter(['test','are','are'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'are': 2, 'test': 1})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.update(iter(['is','are']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'are': 3, 'is': 1, 'test': 1})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_grams(listTokens, n):\n",
    "    shiftedTokens = (listTokens[i:] for i in range(n))\n",
    "    tupleNGrams = zip(*shiftedTokens)\n",
    "    return (\" \".join(i) for i in tupleNGrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def range_ngrams(listTokens, ngram_range):\n",
    "    return chain(*(n_grams(listTokens, i) for i in range(*ngram_range)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all',\n",
       " 'this',\n",
       " 'happened',\n",
       " 'more',\n",
       " 'or',\n",
       " 'less',\n",
       " 'all this',\n",
       " 'this happened',\n",
       " 'happened more',\n",
       " 'more or',\n",
       " 'or less',\n",
       " 'all this happened',\n",
       " 'this happened more',\n",
       " 'happened more or',\n",
       " 'more or less']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_list = ['all', 'this', 'happened', 'more', 'or', 'less']\n",
    "\n",
    "list(range_ngrams(input_list, (1,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'all this'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(('all', 'this'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(*((el for j,el in enumerate(it) if j>=i) for i in range(2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-30c25a04d394>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfind_ngrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hey'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'how'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'are'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'you'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "for a,b in find_ngrams(['hey','how','are','you'], 2):\n",
    "    print (a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-48-f6eeef96d2ec>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-48-f6eeef96d2ec>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    for i in ['hey','how','are','you']:\u001b[0m\n\u001b[0m                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "for i in ['hey','how','are','you']:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num parameters in model: 400092\n",
      "Train on 3563 samples, validate on 188 samples\n",
      "Epoch: 0. Loss: 0.2861010432243347. Acc: 0.8556666666666667.\n",
      "Stoping at epoch:12 with patience:10. Best:0.8655.\n",
      "CPU times: user 53.5 s, sys: 840 ms, total: 54.3 s\n",
      "Wall time: 54.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer(train,callbacks=callbacks,validSize=validation_size,num_workers=num_workers,epochs=max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num parameters in model: 40000092\n",
      "Train on 3563 samples, validate on 188 samples\n",
      "Epoch: 0. Loss: 0.533701479434967. Acc: 0.7716666666666666.\n",
      "Epoch: 1. Loss: 0.24999885261058807. Acc: 0.782.\n",
      "Epoch: 2. Loss: 0.13356901705265045. Acc: 0.794.\n",
      "Epoch: 3. Loss: 0.20342786610126495. Acc: 0.7993333333333333.\n",
      "Epoch: 4. Loss: 0.34462618827819824. Acc: 0.7976666666666666.\n",
      "Epoch: 5. Loss: 0.09012521803379059. Acc: 0.7925.\n",
      "Epoch: 6. Loss: 0.14708219468593597. Acc: 0.7915.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Hash-Embeddings/evaluate/pipeline/trainer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, trainDataset, validDataset, validSize, callbacks, batch_size, epochs, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Hash-Embeddings/venv/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer(train,callbacks=callbacks,validSize=validation_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(train.classes)\n",
    "model = ModelNoDict(max_features,embedding_size,num_classes,isHash=False,num_buckets=num_buckets)\n",
    "trainer = Trainer(model)\n",
    "callbacks = [EarlyStopping(patience=param['patience'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num parameters in model: 200000084\n",
      "Train on 3563 samples, validate on 188 samples\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/Desktop/Hash-Embeddings/venv/lib/python3.6/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1229\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1230\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1231\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Hash-Embeddings/evaluate/pipeline/trainer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, trainDataset, validDataset, validSize, callbacks, batch_size, epochs, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Hash-Embeddings/venv/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/Desktop/Hash-Embeddings/venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   1827\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1828\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1829\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/Desktop/Hash-Embeddings/venv/lib/python3.6/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'zmq.backend.cython.message.Frame.__dealloc__'\n",
      "Traceback (most recent call last):\n",
      "  File \"zmq/backend/cython/checkrc.pxd\", line 12, in zmq.backend.cython.checkrc._check_rc\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Hash-Embeddings/evaluate/pipeline/trainer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, trainDataset, validDataset, validSize, callbacks, batch_size, epochs, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Hash-Embeddings/venv/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mbias_correction1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer(train,callbacks=callbacks,validSize=validation_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_list = 'test the ngrams'*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('test',), ('the',), ('ngrams',), ('generator',), ('test', 'the'), ('the', 'ngrams'), ('ngrams', 'generator'), ('test', 'the', 'ngrams'), ('the', 'ngrams', 'generator')]\n"
     ]
    }
   ],
   "source": [
    "print(list(range_ngrams(input_list, ngramRange=(1,4))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def n_grams(seq, n=1):\n",
    "    \"\"\"Returns an itirator over the n-grams given a listTokens\"\"\"\n",
    "    shiftToken = lambda i: (el for j,el in enumerate(seq) if j>=i)\n",
    "    shiftedTokens = (shiftToken(i) for i in range(n))\n",
    "    tupleNGrams = zip(*shiftedTokens)\n",
    "    return tupleNGrams # if join in generator : (\" \".join(i) for i in tupleNGrams)\n",
    "\n",
    "def range_ngrams(listTokens, ngramRange=(1,2)):\n",
    "    \"\"\"Returns an itirator over all n-grams for n in range(ngramRange) given a listTokens.\"\"\"\n",
    "    return chain(*(n_grams(listTokens, i) for i in range(*ngramRange)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('all',), ('this',), ('happened',), ('more',), ('or',), ('less',), ('all', 'this'), ('this', 'happened'), ('happened', 'more'), ('more', 'or'), ('or', 'less')]\n"
     ]
    }
   ],
   "source": [
    "print(list(range_ngrams(input_list, ngramRange=(1,3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.32 ms ± 241 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "input_list = 'test the ngrams interator vs nltk '*10**6\n",
    "nltk.ngrams(input_list,n=1)\n",
    "nltk.ngrams(input_list,n=2)\n",
    "nltk.ngrams(input_list,n=3)\n",
    "nltk.ngrams(input_list,n=4)\n",
    "nltk.ngrams(input_list,n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.18 ms ± 234 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "input_list = 'test the ngrams interator vs nltk '*10**6\n",
    "range_ngrams(input_list, ngramRange=(1,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num parameters in model: 2000084\n",
      "Train on 3375 samples, validate on 375 samples\n",
      "Epoch: 0. Loss: 0.16543151438236237. Acc: 0.9149166666666667.\n",
      "Epoch: 1. Loss: 0.2636685371398926. Acc: 0.9113333333333333.\n",
      "Epoch: 2. Loss: 0.11431225389242172. Acc: 0.90375.\n",
      "Stoping at epoch:2 with patience:2. Best:0.9149166666666667.\n",
      "CPU times: user 5min 55s, sys: 1.13 s, total: 5min 56s\n",
      "Wall time: 5min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer(train,callbacks=callbacks,validSize=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.init import normal\n",
    "\n",
    "class HashEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_embeddings, embedding_dim, num_buckets=None, num_hashes=2, train_sharedEmbed=True,\n",
    "                 train_weight=True, append_weight=True, aggregation_mode='sum', mask_zero=False,seed=None):\n",
    "        super(HashEmbedding, self).__init__()\n",
    "        \n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_hashes = num_hashes\n",
    "        defaultNBuckets = (num_embeddings * self.num_hashes)//(self.embedding_dim)\n",
    "        self.num_buckets = num_buckets - 1 if num_buckets is not None else defaultNBuckets\n",
    "        self.train_sharedEmbed = train_sharedEmbed\n",
    "        self.train_weight = train_weight\n",
    "        self.append_weight = append_weight\n",
    "        self.padding_idx = 0 if mask_zero else None\n",
    "        self.seed = seed\n",
    "        \n",
    "        self.importance_weights = nn.Embedding(self.num_embeddings,\n",
    "                                              self.num_hashes)\n",
    "        self.shared_embeddings = nn.Embedding(self.num_buckets + 1,\n",
    "                                            self.embedding_dim,\n",
    "                                            padding_idx=self.padding_idx)\n",
    "        self.hashes = None\n",
    "        \n",
    "        if aggregation_mode == 'sum':\n",
    "            self.aggregate = lambda x: torch.sum(x, dim=-1)\n",
    "        elif aggregation_mode == 'concatenate':\n",
    "            # little bit quicker than permute/contiguous/view\n",
    "            self.aggregate = lambda x: torch.cat([x[:,:,:,i] for i in range(self.num_hashes)], dim=-1) \n",
    "        elif aggregation_mode == 'mean':\n",
    "            self.aggregate = lambda x: torch.mean(x, dim=-1)\n",
    "        else:\n",
    "            raise ValueError('unknown aggregation function {}'.format(aggregation_mode))\n",
    "        \n",
    "        self.output_dim = self.embedding_dim \n",
    "        if aggregation_mode == \"concatenate\":\n",
    "            self.output_dim *= self.num_hashes\n",
    "        if self.append_weight:\n",
    "            self.output_dim += self.num_hashes\n",
    "            \n",
    "        self.reset_parameters()   \n",
    "        \n",
    "    def reset_parameters(self,\n",
    "                        init_shared=lambda x: normal(x,std=0.1),\n",
    "                        init_importance=lambda x: normal(x,std=0.0005)):\n",
    "        \"\"\"Resets the trainable parameters.\"\"\"\n",
    "        def set_constant_row(parameters,iRow=0,value=0):\n",
    "            \"\"\"Return `parameters` with row `iRow` as s constant `value`.\"\"\"\n",
    "            data = parameters.data\n",
    "            data[iRow,:] = value\n",
    "            return torch.nn.Parameter(data,requires_grad=parameters.requires_grad)\n",
    "\n",
    "        np.random.seed(self.seed)\n",
    "        if self.seed is not None:\n",
    "            torch.manual_seed(self.seed)\n",
    "\n",
    "        self.shared_embeddings.weight = init_shared(self.shared_embeddings.weight)\n",
    "        self.importance_weights.weight = init_importance(self.importance_weights.weight)\n",
    "\n",
    "        if self.padding_idx is not None:\n",
    "            # Unfortunately has to set weight to 0 even when paddingIdx = 0\n",
    "            self.shared_embeddings.weight = set_constant_row(self.shared_embeddings.weight)\n",
    "            self.importance_weights.weight = set_constant_row(self.importance_weights.weight)\n",
    "\n",
    "        self.shared_embeddings.weight.requires_grad = self.train_sharedEmbed\n",
    "        self.importance_weights.weight.requires_grad = self.train_weight\n",
    "\n",
    "        self.hashes = torch.from_numpy((np.random.randint(0, 2 ** 30,\n",
    "                                                          size=(self.num_embeddings, self.num_hashes)\n",
    "                                                         ) % self.num_buckets) + 1 \n",
    "                                      ).type(torch.LongTensor)\n",
    "        \n",
    "    def _idx_hash(self, inputs, maxOutput, mask_zero=True):\n",
    "        r\"\"\"Hash function for integers used to map indices of different sizes.\n",
    "        \n",
    "        Args:\n",
    "            inputs (torch.Tensor): indices to hash.\n",
    "            maxOutput (int): maximum integer to output. I.e size of table to access.\n",
    "            mask_zero (bool,optional): whether should only map zero input to zero.\n",
    "            \n",
    "        To Do:\n",
    "            Should enable :math:`\\hat{D} \\neq D_1`.\n",
    "        \"\"\"  \n",
    "        if mask_zero:\n",
    "            idx_zero = inputs == 0\n",
    "            # shouldn't map non zero vectors to 0\n",
    "            inputs = inputs%(maxOutput-1) + 1\n",
    "            inputs[idx_zero] = 0\n",
    "            return inputs\n",
    "        else:\n",
    "            return inputs%maxOutput\n",
    "            \n",
    "    def forward(self, input):  \n",
    "        idx_hashes = self._idx_hash(input,self.num_embeddings,mask_zero=self.padding_idx is not None)\n",
    "        idx_importance_weights = self._idx_hash(input,self.num_embeddings,mask_zero=False)\n",
    "        idx_shared_embeddings = self.hashes[idx_hashes.data.cpu(),:]\n",
    "        \n",
    "        shared_embedding = torch.stack([self.shared_embeddings(idx_shared_embeddings[:,:,iHash]) \n",
    "                                        for iHash in range(self.num_hashes)], dim=-1)\n",
    "        importance_weight = self.importance_weights(idx_importance_weights)\n",
    "        importance_weight = importance_weight.unsqueeze(-2)\n",
    "        word_embedding = self.aggregate(importance_weight*shared_embedding)\n",
    "        if self.append_weight:\n",
    "            # concateates the vector with the weights\n",
    "            word_embedding = torch.cat([word_embedding,importance_weight.squeeze(-2)],dim=-1) \n",
    "        return word_embedding\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=HashEmbedding(10,3,5,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    2     3\n",
       "    4     3\n",
       "    4     4\n",
       "    1     1\n",
       "    4     1\n",
       "    1     2\n",
       "    1     1\n",
       "    2     3\n",
       "    1     4\n",
       "    1     3\n",
       "[torch.LongTensor of size 10x2]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[lambda x : 2*x,lambda x : 3*x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "61, 67, 71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 139,\n",
    "    149, 151, 157, 163, 167, 173, 179, 181, 191, 193, 197, 199, 211, 223, 227,\n",
    "    229, 233, 239, 241, 251, 257, 263, 269, 271, 277, 281, 283, 293, 307, 311,\n",
    "    313, 317, 331, 337, 347, 349, 353, 359, 367, 373, 379, 383, 389, 397, 401,\n",
    "    409, 419, 421, 431, 433, 439, 443, 449, 457, 461, 463, 467, 479, 487, 491,\n",
    "    499, 503, 509, 521, 523, 541, 547, 557, 563, 569, 571, 577, 587, 593, 599,\n",
    "    601, 607, 613, 617, 619, 631, 641, 643, 647, 653, 659, 661, 673, 677, 683,\n",
    "    691, 701, 709, 719, 727, 733, 739, 743, 751, 757, 761, 769, 773, 787, 797,\n",
    "    809, 811, 821, 823, 827, 829, 839, 853, 857, 859, 863, 877, 881, 883, 887,\n",
    "    907, 911, 919, 929, 937, 941, 947, 953, 967, 971, 977, 983, 991, 997)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " 1, 11, 13, 17, 19, 23, 29, 31, 37, 41,\n",
    "   43, 47, 53, 59, 61, 67, 71, 73, 79, 83,\n",
    "   89, 97,101,103,107,109,113,121,127,131,\n",
    "  137,139,143,149,151,157,163,167,169,173,\n",
    "  179,181,187,191,193,197,199,209"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_prim(100000007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_prime(n):\n",
    "    def is_prime(x):\n",
    "        for i in range(2,int(np.sqrt(x))):\n",
    "            if x % i == 0:\n",
    "                return False\n",
    "        return True\n",
    "            \n",
    "    while not is_prime(n):\n",
    "        n += 1\n",
    "    \n",
    "    return n\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000007"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_prime(10**8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def universal_hashes(a,b,p=199,m=127):\n",
    "    return lambda x: ((a*x+b)%p)%m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "786803 100\n",
      "387521 26976\n",
      "273270 124644\n"
     ]
    }
   ],
   "source": [
    "n = 10**2\n",
    "\n",
    "p=next_prime(n*np.random.randint(10000))\n",
    "m=next_prime(n)\n",
    "m=n\n",
    "print(p,m)\n",
    "a = np.random.randint(1,p)\n",
    "b = np.random.randint(0,p)\n",
    "h1 = universal_hashes(a,b,p=p,m=m)\n",
    "print(a,b)\n",
    "a = np.random.randint(1,p)\n",
    "b = np.random.randint(0,p)\n",
    "h2 = universal_hashes(a,b,p=p,m=m)\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e2b1470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "arr1 = np.array([h1(i) for i in range(10**7)])\n",
    "_=plt.hist(arr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10eb91780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "arr2 = np.array([h2(i) for i in range(10**7)])\n",
    "_=plt.hist(arr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({27311: 101,\n",
       "         34893: 101,\n",
       "         42475: 101,\n",
       "         50057: 101,\n",
       "         57639: 101,\n",
       "         65221: 101,\n",
       "         72803: 101,\n",
       "         80385: 101,\n",
       "         87967: 101,\n",
       "         95549: 101,\n",
       "         3128: 101,\n",
       "         10710: 101,\n",
       "         18292: 101,\n",
       "         25874: 101,\n",
       "         33456: 101,\n",
       "         41038: 101,\n",
       "         48620: 101,\n",
       "         56202: 101,\n",
       "         63784: 101,\n",
       "         71366: 101,\n",
       "         78948: 101,\n",
       "         86530: 101,\n",
       "         94112: 101,\n",
       "         1717: 100,\n",
       "         9299: 100,\n",
       "         16881: 100,\n",
       "         24463: 100,\n",
       "         32045: 100,\n",
       "         39627: 100,\n",
       "         47209: 100,\n",
       "         54791: 100,\n",
       "         62373: 100,\n",
       "         69955: 100,\n",
       "         77537: 100,\n",
       "         85119: 100,\n",
       "         92701: 100,\n",
       "         280: 100,\n",
       "         7862: 100,\n",
       "         15444: 100,\n",
       "         23026: 100,\n",
       "         30608: 100,\n",
       "         38190: 100,\n",
       "         45772: 100,\n",
       "         53354: 100,\n",
       "         60936: 100,\n",
       "         68518: 100,\n",
       "         76100: 100,\n",
       "         83682: 100,\n",
       "         91264: 100,\n",
       "         98846: 100,\n",
       "         6425: 100,\n",
       "         14007: 100,\n",
       "         21589: 100,\n",
       "         29171: 100,\n",
       "         36753: 100,\n",
       "         44335: 100,\n",
       "         51917: 100,\n",
       "         59499: 100,\n",
       "         67081: 100,\n",
       "         74663: 100,\n",
       "         82245: 100,\n",
       "         89827: 100,\n",
       "         97409: 100,\n",
       "         4988: 100,\n",
       "         12570: 100,\n",
       "         20152: 100,\n",
       "         27734: 100,\n",
       "         35316: 100,\n",
       "         42898: 100,\n",
       "         50480: 100,\n",
       "         58062: 100,\n",
       "         65644: 100,\n",
       "         73226: 100,\n",
       "         80808: 100,\n",
       "         88390: 100,\n",
       "         95972: 100,\n",
       "         3551: 100,\n",
       "         11133: 100,\n",
       "         18715: 100,\n",
       "         26297: 100,\n",
       "         33879: 100,\n",
       "         41461: 100,\n",
       "         49043: 100,\n",
       "         56625: 100,\n",
       "         64207: 100,\n",
       "         71789: 100,\n",
       "         79371: 100,\n",
       "         86953: 100,\n",
       "         94535: 100,\n",
       "         2114: 101,\n",
       "         9696: 101,\n",
       "         17278: 101,\n",
       "         24860: 101,\n",
       "         32442: 101,\n",
       "         40024: 101,\n",
       "         47606: 101,\n",
       "         55188: 101,\n",
       "         62770: 101,\n",
       "         70352: 101,\n",
       "         77934: 101,\n",
       "         85516: 101,\n",
       "         93098: 101,\n",
       "         677: 101,\n",
       "         8259: 101,\n",
       "         15841: 101,\n",
       "         23423: 101,\n",
       "         31005: 101,\n",
       "         38587: 101,\n",
       "         46169: 101,\n",
       "         53751: 101,\n",
       "         61333: 101,\n",
       "         68915: 101,\n",
       "         76497: 101,\n",
       "         84079: 101,\n",
       "         91661: 101,\n",
       "         99243: 101,\n",
       "         6822: 101,\n",
       "         14404: 101,\n",
       "         21986: 101,\n",
       "         29568: 101,\n",
       "         37150: 101,\n",
       "         44732: 101,\n",
       "         52314: 101,\n",
       "         59896: 101,\n",
       "         67478: 101,\n",
       "         75060: 101,\n",
       "         82642: 101,\n",
       "         90224: 101,\n",
       "         97806: 101,\n",
       "         5385: 101,\n",
       "         12967: 101,\n",
       "         20549: 101,\n",
       "         28131: 101,\n",
       "         35713: 101,\n",
       "         43295: 101,\n",
       "         50877: 101,\n",
       "         58459: 101,\n",
       "         66041: 101,\n",
       "         73623: 101,\n",
       "         81205: 101,\n",
       "         88787: 101,\n",
       "         96369: 101,\n",
       "         3974: 100,\n",
       "         11556: 100,\n",
       "         19138: 100,\n",
       "         26720: 100,\n",
       "         34302: 100,\n",
       "         41884: 100,\n",
       "         49466: 100,\n",
       "         57048: 100,\n",
       "         64630: 100,\n",
       "         72212: 100,\n",
       "         79794: 100,\n",
       "         87376: 100,\n",
       "         94958: 100,\n",
       "         2537: 100,\n",
       "         10119: 100,\n",
       "         17701: 100,\n",
       "         25283: 100,\n",
       "         32865: 100,\n",
       "         40447: 100,\n",
       "         48029: 100,\n",
       "         55611: 100,\n",
       "         63193: 100,\n",
       "         70775: 100,\n",
       "         78357: 100,\n",
       "         85939: 100,\n",
       "         93521: 100,\n",
       "         1100: 100,\n",
       "         8682: 100,\n",
       "         16264: 100,\n",
       "         23846: 100,\n",
       "         31428: 100,\n",
       "         39010: 100,\n",
       "         46592: 100,\n",
       "         54174: 100,\n",
       "         61756: 100,\n",
       "         69338: 100,\n",
       "         76920: 100,\n",
       "         84502: 100,\n",
       "         92084: 100,\n",
       "         99666: 100,\n",
       "         7245: 100,\n",
       "         14827: 100,\n",
       "         22409: 100,\n",
       "         29991: 100,\n",
       "         37573: 100,\n",
       "         45155: 100,\n",
       "         52737: 100,\n",
       "         60319: 100,\n",
       "         67901: 100,\n",
       "         75483: 100,\n",
       "         83065: 100,\n",
       "         90647: 100,\n",
       "         98229: 100,\n",
       "         5808: 100,\n",
       "         13390: 100,\n",
       "         20972: 100,\n",
       "         28554: 100,\n",
       "         36136: 100,\n",
       "         43718: 100,\n",
       "         51300: 100,\n",
       "         58882: 100,\n",
       "         66464: 100,\n",
       "         74046: 100,\n",
       "         81628: 100,\n",
       "         89210: 100,\n",
       "         96792: 100,\n",
       "         4371: 101,\n",
       "         11953: 101,\n",
       "         19535: 101,\n",
       "         27117: 101,\n",
       "         34699: 101,\n",
       "         42281: 101,\n",
       "         49863: 101,\n",
       "         57445: 101,\n",
       "         65027: 101,\n",
       "         72609: 101,\n",
       "         80191: 101,\n",
       "         87773: 101,\n",
       "         95355: 101,\n",
       "         2934: 101,\n",
       "         10516: 101,\n",
       "         18098: 101,\n",
       "         25680: 101,\n",
       "         33262: 101,\n",
       "         40844: 101,\n",
       "         48426: 101,\n",
       "         56008: 101,\n",
       "         63590: 101,\n",
       "         71172: 101,\n",
       "         78754: 101,\n",
       "         86336: 101,\n",
       "         93918: 101,\n",
       "         1497: 101,\n",
       "         9079: 101,\n",
       "         16661: 101,\n",
       "         24243: 101,\n",
       "         31825: 101,\n",
       "         39407: 101,\n",
       "         46989: 101,\n",
       "         54571: 101,\n",
       "         62153: 101,\n",
       "         69735: 101,\n",
       "         77317: 101,\n",
       "         84899: 101,\n",
       "         92481: 101,\n",
       "         60: 101,\n",
       "         7642: 101,\n",
       "         15224: 101,\n",
       "         22806: 101,\n",
       "         30388: 101,\n",
       "         37970: 101,\n",
       "         45552: 101,\n",
       "         53134: 101,\n",
       "         60716: 101,\n",
       "         68298: 101,\n",
       "         75880: 101,\n",
       "         83462: 101,\n",
       "         91044: 101,\n",
       "         98626: 101,\n",
       "         6231: 100,\n",
       "         13813: 100,\n",
       "         21395: 100,\n",
       "         28977: 100,\n",
       "         36559: 100,\n",
       "         44141: 100,\n",
       "         51723: 100,\n",
       "         59305: 100,\n",
       "         66887: 100,\n",
       "         74469: 100,\n",
       "         82051: 100,\n",
       "         89633: 100,\n",
       "         97215: 100,\n",
       "         4794: 100,\n",
       "         12376: 100,\n",
       "         19958: 100,\n",
       "         27540: 100,\n",
       "         35122: 100,\n",
       "         42704: 100,\n",
       "         50286: 100,\n",
       "         57868: 100,\n",
       "         65450: 100,\n",
       "         73032: 100,\n",
       "         80614: 100,\n",
       "         88196: 100,\n",
       "         95778: 100,\n",
       "         3357: 100,\n",
       "         10939: 100,\n",
       "         18521: 100,\n",
       "         26103: 100,\n",
       "         33685: 100,\n",
       "         41267: 100,\n",
       "         48849: 100,\n",
       "         56431: 100,\n",
       "         64013: 100,\n",
       "         71595: 100,\n",
       "         79177: 100,\n",
       "         86759: 100,\n",
       "         94341: 100,\n",
       "         1920: 100,\n",
       "         9502: 100,\n",
       "         17084: 100,\n",
       "         24666: 100,\n",
       "         32248: 100,\n",
       "         39830: 100,\n",
       "         47412: 100,\n",
       "         54994: 100,\n",
       "         62576: 100,\n",
       "         70158: 100,\n",
       "         77740: 100,\n",
       "         85322: 100,\n",
       "         92904: 100,\n",
       "         483: 100,\n",
       "         8065: 100,\n",
       "         15647: 100,\n",
       "         23229: 100,\n",
       "         30811: 100,\n",
       "         38393: 100,\n",
       "         45975: 100,\n",
       "         53557: 100,\n",
       "         61139: 100,\n",
       "         68721: 100,\n",
       "         76303: 100,\n",
       "         83885: 100,\n",
       "         91467: 100,\n",
       "         99049: 100,\n",
       "         6628: 101,\n",
       "         14210: 101,\n",
       "         21792: 101,\n",
       "         29374: 101,\n",
       "         36956: 101,\n",
       "         44538: 101,\n",
       "         52120: 101,\n",
       "         59702: 101,\n",
       "         67284: 101,\n",
       "         74866: 101,\n",
       "         82448: 101,\n",
       "         90030: 101,\n",
       "         97612: 101,\n",
       "         5191: 101,\n",
       "         12773: 101,\n",
       "         20355: 101,\n",
       "         27937: 101,\n",
       "         35519: 101,\n",
       "         43101: 101,\n",
       "         50683: 101,\n",
       "         58265: 101,\n",
       "         65847: 101,\n",
       "         73429: 101,\n",
       "         81011: 101,\n",
       "         88593: 101,\n",
       "         96175: 101,\n",
       "         3754: 101,\n",
       "         11336: 101,\n",
       "         18918: 101,\n",
       "         26500: 101,\n",
       "         34082: 101,\n",
       "         41664: 101,\n",
       "         49246: 101,\n",
       "         56828: 101,\n",
       "         64410: 101,\n",
       "         71992: 101,\n",
       "         79574: 101,\n",
       "         87156: 101,\n",
       "         94738: 101,\n",
       "         2317: 101,\n",
       "         9899: 101,\n",
       "         17481: 101,\n",
       "         25063: 101,\n",
       "         32645: 101,\n",
       "         40227: 101,\n",
       "         47809: 101,\n",
       "         55391: 101,\n",
       "         62973: 101,\n",
       "         70555: 101,\n",
       "         78137: 101,\n",
       "         85719: 101,\n",
       "         93301: 101,\n",
       "         906: 100,\n",
       "         8488: 100,\n",
       "         16070: 100,\n",
       "         23652: 100,\n",
       "         31234: 100,\n",
       "         38816: 100,\n",
       "         46398: 100,\n",
       "         53980: 100,\n",
       "         61562: 100,\n",
       "         69144: 100,\n",
       "         76726: 100,\n",
       "         84308: 100,\n",
       "         91890: 100,\n",
       "         99472: 100,\n",
       "         7051: 100,\n",
       "         14633: 100,\n",
       "         22215: 100,\n",
       "         29797: 100,\n",
       "         37379: 100,\n",
       "         44961: 100,\n",
       "         52543: 100,\n",
       "         60125: 100,\n",
       "         67707: 100,\n",
       "         75289: 100,\n",
       "         82871: 100,\n",
       "         90453: 100,\n",
       "         98035: 100,\n",
       "         5614: 100,\n",
       "         13196: 100,\n",
       "         20778: 100,\n",
       "         28360: 100,\n",
       "         35942: 100,\n",
       "         43524: 100,\n",
       "         51106: 100,\n",
       "         58688: 100,\n",
       "         66270: 100,\n",
       "         73852: 100,\n",
       "         81434: 100,\n",
       "         89016: 100,\n",
       "         96598: 100,\n",
       "         4177: 100,\n",
       "         11759: 100,\n",
       "         19341: 100,\n",
       "         26923: 100,\n",
       "         34505: 100,\n",
       "         42087: 100,\n",
       "         49669: 100,\n",
       "         57251: 100,\n",
       "         64833: 100,\n",
       "         72415: 100,\n",
       "         79997: 100,\n",
       "         87579: 100,\n",
       "         95161: 100,\n",
       "         2740: 100,\n",
       "         10322: 100,\n",
       "         17904: 100,\n",
       "         25486: 100,\n",
       "         33068: 100,\n",
       "         40650: 100,\n",
       "         48232: 100,\n",
       "         55814: 100,\n",
       "         63396: 100,\n",
       "         70978: 100,\n",
       "         78560: 100,\n",
       "         86142: 100,\n",
       "         93724: 100,\n",
       "         1303: 101,\n",
       "         8885: 101,\n",
       "         16467: 101,\n",
       "         24049: 101,\n",
       "         31631: 101,\n",
       "         39213: 101,\n",
       "         46795: 101,\n",
       "         54377: 101,\n",
       "         61959: 101,\n",
       "         69541: 101,\n",
       "         77123: 101,\n",
       "         84705: 101,\n",
       "         92287: 101,\n",
       "         99869: 101,\n",
       "         7448: 101,\n",
       "         15030: 101,\n",
       "         22612: 101,\n",
       "         30194: 101,\n",
       "         37776: 101,\n",
       "         45358: 101,\n",
       "         52940: 101,\n",
       "         60522: 101,\n",
       "         68104: 101,\n",
       "         75686: 101,\n",
       "         83268: 101,\n",
       "         90850: 101,\n",
       "         98432: 101,\n",
       "         6011: 101,\n",
       "         13593: 101,\n",
       "         21175: 101,\n",
       "         28757: 101,\n",
       "         36339: 101,\n",
       "         43921: 101,\n",
       "         51503: 101,\n",
       "         59085: 101,\n",
       "         66667: 101,\n",
       "         74249: 101,\n",
       "         81831: 101,\n",
       "         89413: 101,\n",
       "         96995: 101,\n",
       "         4574: 101,\n",
       "         12156: 101,\n",
       "         19738: 101,\n",
       "         27320: 101,\n",
       "         34902: 101,\n",
       "         42484: 101,\n",
       "         50066: 101,\n",
       "         57648: 101,\n",
       "         65230: 101,\n",
       "         72812: 101,\n",
       "         80394: 101,\n",
       "         87976: 101,\n",
       "         95558: 101,\n",
       "         3163: 100,\n",
       "         10745: 100,\n",
       "         18327: 100,\n",
       "         25909: 100,\n",
       "         33491: 100,\n",
       "         41073: 100,\n",
       "         48655: 100,\n",
       "         56237: 100,\n",
       "         63819: 100,\n",
       "         71401: 100,\n",
       "         78983: 100,\n",
       "         86565: 100,\n",
       "         94147: 100,\n",
       "         1726: 100,\n",
       "         9308: 100,\n",
       "         16890: 100,\n",
       "         24472: 100,\n",
       "         32054: 100,\n",
       "         39636: 100,\n",
       "         47218: 100,\n",
       "         54800: 100,\n",
       "         62382: 100,\n",
       "         69964: 100,\n",
       "         77546: 100,\n",
       "         85128: 100,\n",
       "         92710: 100,\n",
       "         289: 100,\n",
       "         7871: 100,\n",
       "         15453: 100,\n",
       "         23035: 100,\n",
       "         30617: 100,\n",
       "         38199: 100,\n",
       "         45781: 100,\n",
       "         53363: 100,\n",
       "         60945: 100,\n",
       "         68527: 100,\n",
       "         76109: 100,\n",
       "         83691: 100,\n",
       "         91273: 100,\n",
       "         98855: 100,\n",
       "         6434: 100,\n",
       "         14016: 100,\n",
       "         21598: 100,\n",
       "         29180: 100,\n",
       "         36762: 100,\n",
       "         44344: 100,\n",
       "         51926: 100,\n",
       "         59508: 100,\n",
       "         67090: 100,\n",
       "         74672: 100,\n",
       "         82254: 100,\n",
       "         89836: 100,\n",
       "         97418: 100,\n",
       "         4997: 100,\n",
       "         12579: 100,\n",
       "         20161: 100,\n",
       "         27743: 100,\n",
       "         35325: 100,\n",
       "         42907: 100,\n",
       "         50489: 100,\n",
       "         58071: 100,\n",
       "         65653: 100,\n",
       "         73235: 100,\n",
       "         80817: 100,\n",
       "         88399: 100,\n",
       "         95981: 100,\n",
       "         3560: 101,\n",
       "         11142: 101,\n",
       "         18724: 101,\n",
       "         26306: 101,\n",
       "         33888: 101,\n",
       "         41470: 101,\n",
       "         49052: 101,\n",
       "         56634: 101,\n",
       "         64216: 101,\n",
       "         71798: 101,\n",
       "         79380: 101,\n",
       "         86962: 101,\n",
       "         94544: 101,\n",
       "         2123: 101,\n",
       "         9705: 101,\n",
       "         17287: 101,\n",
       "         24869: 101,\n",
       "         32451: 101,\n",
       "         40033: 101,\n",
       "         47615: 101,\n",
       "         55197: 101,\n",
       "         62779: 101,\n",
       "         70361: 101,\n",
       "         77943: 101,\n",
       "         85525: 101,\n",
       "         93107: 101,\n",
       "         686: 101,\n",
       "         8268: 101,\n",
       "         15850: 101,\n",
       "         23432: 101,\n",
       "         31014: 101,\n",
       "         38596: 101,\n",
       "         46178: 101,\n",
       "         53760: 101,\n",
       "         61342: 101,\n",
       "         68924: 101,\n",
       "         76506: 101,\n",
       "         84088: 101,\n",
       "         91670: 101,\n",
       "         99252: 101,\n",
       "         6831: 101,\n",
       "         14413: 101,\n",
       "         21995: 101,\n",
       "         29577: 101,\n",
       "         37159: 101,\n",
       "         44741: 101,\n",
       "         52323: 101,\n",
       "         59905: 101,\n",
       "         67487: 101,\n",
       "         75069: 101,\n",
       "         82651: 101,\n",
       "         90233: 101,\n",
       "         97815: 101,\n",
       "         5420: 100,\n",
       "         13002: 100,\n",
       "         20584: 100,\n",
       "         28166: 100,\n",
       "         35748: 100,\n",
       "         43330: 100,\n",
       "         50912: 100,\n",
       "         58494: 100,\n",
       "         66076: 100,\n",
       "         73658: 100,\n",
       "         81240: 100,\n",
       "         88822: 100,\n",
       "         96404: 100,\n",
       "         3983: 100,\n",
       "         11565: 100,\n",
       "         19147: 100,\n",
       "         26729: 100,\n",
       "         34311: 100,\n",
       "         41893: 100,\n",
       "         49475: 100,\n",
       "         57057: 100,\n",
       "         64639: 100,\n",
       "         72221: 100,\n",
       "         79803: 100,\n",
       "         87385: 100,\n",
       "         94967: 100,\n",
       "         2546: 100,\n",
       "         10128: 100,\n",
       "         17710: 100,\n",
       "         25292: 100,\n",
       "         32874: 100,\n",
       "         40456: 100,\n",
       "         48038: 100,\n",
       "         55620: 100,\n",
       "         63202: 100,\n",
       "         70784: 100,\n",
       "         78366: 100,\n",
       "         85948: 100,\n",
       "         93530: 100,\n",
       "         1109: 100,\n",
       "         8691: 100,\n",
       "         16273: 100,\n",
       "         23855: 100,\n",
       "         31437: 100,\n",
       "         39019: 100,\n",
       "         46601: 100,\n",
       "         54183: 100,\n",
       "         61765: 100,\n",
       "         69347: 100,\n",
       "         76929: 100,\n",
       "         84511: 100,\n",
       "         92093: 100,\n",
       "         99675: 100,\n",
       "         7254: 100,\n",
       "         14836: 100,\n",
       "         22418: 100,\n",
       "         30000: 100,\n",
       "         37582: 100,\n",
       "         45164: 100,\n",
       "         52746: 100,\n",
       "         60328: 100,\n",
       "         67910: 100,\n",
       "         75492: 100,\n",
       "         83074: 100,\n",
       "         90656: 100,\n",
       "         98238: 100,\n",
       "         5817: 101,\n",
       "         13399: 101,\n",
       "         20981: 101,\n",
       "         28563: 101,\n",
       "         36145: 101,\n",
       "         43727: 101,\n",
       "         51309: 101,\n",
       "         58891: 101,\n",
       "         66473: 101,\n",
       "         74055: 101,\n",
       "         81637: 101,\n",
       "         89219: 101,\n",
       "         96801: 101,\n",
       "         4380: 101,\n",
       "         11962: 101,\n",
       "         19544: 101,\n",
       "         27126: 101,\n",
       "         34708: 101,\n",
       "         42290: 101,\n",
       "         49872: 101,\n",
       "         57454: 101,\n",
       "         65036: 101,\n",
       "         72618: 101,\n",
       "         80200: 101,\n",
       "         87782: 101,\n",
       "         95364: 101,\n",
       "         2943: 101,\n",
       "         10525: 101,\n",
       "         18107: 101,\n",
       "         25689: 101,\n",
       "         33271: 101,\n",
       "         40853: 101,\n",
       "         48435: 101,\n",
       "         56017: 101,\n",
       "         63599: 101,\n",
       "         71181: 101,\n",
       "         78763: 101,\n",
       "         86345: 101,\n",
       "         93927: 101,\n",
       "         1506: 101,\n",
       "         9088: 101,\n",
       "         16670: 101,\n",
       "         24252: 101,\n",
       "         31834: 101,\n",
       "         39416: 101,\n",
       "         46998: 101,\n",
       "         54580: 101,\n",
       "         62162: 101,\n",
       "         69744: 101,\n",
       "         77326: 101,\n",
       "         84908: 101,\n",
       "         92490: 101,\n",
       "         95: 100,\n",
       "         7677: 100,\n",
       "         15259: 100,\n",
       "         22841: 100,\n",
       "         30423: 100,\n",
       "         38005: 100,\n",
       "         45587: 100,\n",
       "         53169: 100,\n",
       "         60751: 100,\n",
       "         68333: 100,\n",
       "         75915: 100,\n",
       "         83497: 100,\n",
       "         91079: 100,\n",
       "         98661: 100,\n",
       "         6240: 100,\n",
       "         13822: 100,\n",
       "         21404: 100,\n",
       "         28986: 100,\n",
       "         36568: 100,\n",
       "         44150: 100,\n",
       "         51732: 100,\n",
       "         59314: 100,\n",
       "         66896: 100,\n",
       "         74478: 100,\n",
       "         82060: 100,\n",
       "         89642: 100,\n",
       "         97224: 100,\n",
       "         4803: 100,\n",
       "         12385: 100,\n",
       "         19967: 100,\n",
       "         27549: 100,\n",
       "         35131: 100,\n",
       "         42713: 100,\n",
       "         50295: 100,\n",
       "         57877: 100,\n",
       "         65459: 100,\n",
       "         73041: 100,\n",
       "         80623: 100,\n",
       "         88205: 100,\n",
       "         95787: 100,\n",
       "         3366: 100,\n",
       "         10948: 100,\n",
       "         18530: 100,\n",
       "         26112: 100,\n",
       "         33694: 100,\n",
       "         41276: 100,\n",
       "         48858: 100,\n",
       "         56440: 100,\n",
       "         64022: 100,\n",
       "         71604: 100,\n",
       "         79186: 100,\n",
       "         86768: 100,\n",
       "         94350: 100,\n",
       "         1929: 100,\n",
       "         9511: 100,\n",
       "         17093: 100,\n",
       "         24675: 100,\n",
       "         32257: 100,\n",
       "         39839: 100,\n",
       "         47421: 100,\n",
       "         55003: 100,\n",
       "         62585: 100,\n",
       "         70167: 100,\n",
       "         77749: 100,\n",
       "         85331: 100,\n",
       "         92913: 100,\n",
       "         492: 101,\n",
       "         8074: 101,\n",
       "         15656: 101,\n",
       "         23238: 101,\n",
       "         30820: 101,\n",
       "         38402: 101,\n",
       "         45984: 101,\n",
       "         53566: 101,\n",
       "         61148: 101,\n",
       "         68730: 101,\n",
       "         76312: 101,\n",
       "         83894: 101,\n",
       "         91476: 101,\n",
       "         99058: 101,\n",
       "         6637: 101,\n",
       "         14219: 101,\n",
       "         21801: 101,\n",
       "         29383: 101,\n",
       "         36965: 101,\n",
       "         44547: 101,\n",
       "         52129: 101,\n",
       "         59711: 101,\n",
       "         67293: 101,\n",
       "         74875: 101,\n",
       "         82457: 101,\n",
       "         90039: 101,\n",
       "         97621: 101,\n",
       "         5200: 101,\n",
       "         12782: 101,\n",
       "         20364: 101,\n",
       "         27946: 101,\n",
       "         35528: 101,\n",
       "         43110: 101,\n",
       "         50692: 101,\n",
       "         58274: 101,\n",
       "         65856: 101,\n",
       "         73438: 101,\n",
       "         81020: 101,\n",
       "         88602: 101,\n",
       "         96184: 101,\n",
       "         3763: 101,\n",
       "         11345: 101,\n",
       "         18927: 101,\n",
       "         26509: 101,\n",
       "         34091: 101,\n",
       "         41673: 101,\n",
       "         49255: 101,\n",
       "         56837: 101,\n",
       "         64419: 101,\n",
       "         72001: 101,\n",
       "         79583: 101,\n",
       "         87165: 101,\n",
       "         94747: 101,\n",
       "         2352: 100,\n",
       "         9934: 100,\n",
       "         17516: 100,\n",
       "         25098: 100,\n",
       "         32680: 100,\n",
       "         40262: 100,\n",
       "         47844: 100,\n",
       "         55426: 100,\n",
       "         63008: 100,\n",
       "         70590: 100,\n",
       "         78172: 100,\n",
       "         85754: 100,\n",
       "         93336: 100,\n",
       "         915: 100,\n",
       "         8497: 100,\n",
       "         16079: 100,\n",
       "         23661: 100,\n",
       "         31243: 100,\n",
       "         38825: 100,\n",
       "         46407: 100,\n",
       "         53989: 100,\n",
       "         61571: 100,\n",
       "         69153: 100,\n",
       "         76735: 100,\n",
       "         84317: 100,\n",
       "         91899: 100,\n",
       "         99481: 100,\n",
       "         7060: 100,\n",
       "         14642: 100,\n",
       "         22224: 100,\n",
       "         29806: 100,\n",
       "         37388: 100,\n",
       "         44970: 100,\n",
       "         52552: 100,\n",
       "         60134: 100,\n",
       "         67716: 100,\n",
       "         75298: 100,\n",
       "         82880: 100,\n",
       "         90462: 100,\n",
       "         98044: 100,\n",
       "         5623: 100,\n",
       "         13205: 100,\n",
       "         20787: 100,\n",
       "         28369: 100,\n",
       "         35951: 100,\n",
       "         43533: 100,\n",
       "         51115: 100,\n",
       "         58697: 100,\n",
       "         66279: 100,\n",
       "         73861: 100,\n",
       "         81443: 100,\n",
       "         89025: 100,\n",
       "         96607: 100,\n",
       "         4186: 100,\n",
       "         11768: 100,\n",
       "         19350: 100,\n",
       "         26932: 100,\n",
       "         34514: 100,\n",
       "         42096: 100,\n",
       "         49678: 100,\n",
       "         57260: 100,\n",
       "         64842: 100,\n",
       "         72424: 100,\n",
       "         80006: 100,\n",
       "         87588: 100,\n",
       "         95170: 100,\n",
       "         2749: 101,\n",
       "         10331: 101,\n",
       "         17913: 101,\n",
       "         25495: 101,\n",
       "         33077: 101,\n",
       "         40659: 101,\n",
       "         48241: 101,\n",
       "         55823: 101,\n",
       "         63405: 101,\n",
       "         70987: 101,\n",
       "         78569: 101,\n",
       "         86151: 101,\n",
       "         93733: 101,\n",
       "         1312: 101,\n",
       "         8894: 101,\n",
       "         16476: 101,\n",
       "         24058: 101,\n",
       "         31640: 101,\n",
       "         39222: 101,\n",
       "         46804: 101,\n",
       "         54386: 101,\n",
       "         61968: 101,\n",
       "         69550: 101,\n",
       "         77132: 101,\n",
       "         84714: 101,\n",
       "         92296: 101,\n",
       "         99878: 101,\n",
       "         7457: 101,\n",
       "         15039: 101,\n",
       "         22621: 101,\n",
       "         30203: 101,\n",
       "         37785: 101,\n",
       "         45367: 101,\n",
       "         52949: 101,\n",
       "         60531: 101,\n",
       "         68113: 101,\n",
       "         75695: 101,\n",
       "         83277: 101,\n",
       "         90859: 101,\n",
       "         98441: 101,\n",
       "         6020: 101,\n",
       "         13602: 101,\n",
       "         21184: 101,\n",
       "         28766: 101,\n",
       "         36348: 101,\n",
       "         43930: 101,\n",
       "         51512: 101,\n",
       "         59094: 101,\n",
       "         66676: 101,\n",
       "         74258: 101,\n",
       "         81840: 101,\n",
       "         89422: 101,\n",
       "         97004: 101,\n",
       "         4609: 100,\n",
       "         12191: 100,\n",
       "         19773: 100,\n",
       "         27355: 100,\n",
       "         34937: 100,\n",
       "         42519: 100,\n",
       "         50101: 100,\n",
       "         57683: 100,\n",
       "         65265: 100,\n",
       "         72847: 100,\n",
       "         80429: 100,\n",
       "         88011: 100,\n",
       "         95593: 100,\n",
       "         3172: 100,\n",
       "         10754: 100,\n",
       "         18336: 100,\n",
       "         25918: 100,\n",
       "         33500: 100,\n",
       "         41082: 100,\n",
       "         48664: 100,\n",
       "         56246: 100,\n",
       "         63828: 100,\n",
       "         71410: 100,\n",
       "         78992: 100,\n",
       "         86574: 100,\n",
       "         94156: 100,\n",
       "         1735: 100,\n",
       "         ...})"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(arr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
