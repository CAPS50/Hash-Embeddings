{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:46:52.264023Z",
     "start_time": "2018-01-18T15:46:52.077395Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from evaluate.load.helpers import *\n",
    "from evaluate.load.dataset import *\n",
    "from evaluate.pipeline.model import *\n",
    "from evaluate.pipeline.trainer import *\n",
    "from evaluate.pipeline.helpers import *\n",
    "#pip install git+https://github.com/ncullen93/torchsample.git\n",
    "\n",
    "from torchsample.modules import ModuleTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pair(parser,arg,name,types=(int,int)):\n",
    "    if arg[0] == \"None\":\n",
    "        arg = None\n",
    "    if arg is not None and len(arg) != 2:\n",
    "        raise parser.error(\"{} has to be None or of length 2.\".format(name))\n",
    "    if arg is not None:\n",
    "        try:\n",
    "            arg[0] = types[0](arg[0])\n",
    "            arg[1] = types[1](arg[1])\n",
    "        except ValueError:\n",
    "            raise parser.error(\"{} should be of type {}\".format(name,types))\n",
    "    return arg\n",
    "    \n",
    "\n",
    "def parse_arguments(l):\n",
    "    \"\"\"Parses the arguments from the command line.\"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"PyTorch implementation and evaluation of HashEmbeddings, which uses multiple hashes to efficiently approximate an Embedding layer.\")\n",
    "    \n",
    "    # Dataset options\n",
    "    data = parser.add_argument_group('Dataset options')\n",
    "    datasets = ['ag','amazon','dbpedia','sogou','yahoo','yelp','yelp-polarity']\n",
    "    data.add_argument('-d','--dataset', help='path to training data csv', default='ag', choices=datasets)\n",
    "\n",
    "    # Learning options\n",
    "    learn = parser.add_argument_group('Learning options')\n",
    "    learn.add_argument('--no-shuffle', action='store_true', default=False, help='Disables shuffling batches when training.')\n",
    "    learn.add_argument('--no-checkpoint', action='store_true', default=False, help='Disables model checkpoint. I.e saving best model based on validation loss.')\n",
    "    learn.add_argument('-e','--epochs', type=int, default=300, help='Maximum number of epochs to run for.')\n",
    "    learn.add_argument('-b','--batch-size', type=int, default=64, help='Batch size for training.')\n",
    "    learn.add_argument('-v','--validation-size', type=float, default=0.05, help='Percentage of training set to use as validation.')\n",
    "    learn.add_argument('-s','--seed', type=int, default=123, help='Random seed.')\n",
    "    learn.add_argument('-p','--patience', type=int, default=10, help='Patience if early stopping. None means no early stopping.')\n",
    "    learn.add_argument('-V','--verbose', type=int, default=3, help='Verbosity in [0,3].')\n",
    "    learn.add_argument('-P','--plateau-reduce-lr', metavar=('PATIENCE','FACTOR'), nargs='*', default=[5,0.5], help='If specified, if loss did not improve since PATIENCE epochs then multiply lr by FACTOR. [None,None] means no reducing of lr on plateau.')\n",
    "    \n",
    "    # Device options\n",
    "    device = parser.add_argument_group('Device options')\n",
    "    device.add_argument('--no-cuda', action='store_true', default=False, help='Disables CUDA training, even when have one.')\n",
    "    device.add_argument('-w','--num-workers', type=int, default=0, help='Number of subprocesses used for data loading.')\n",
    "\n",
    "    # Featurizing options\n",
    "    feature = parser.add_argument_group('Featurizing options')\n",
    "    feature.add_argument('--dictionnary', action='store_true', default=False, help='Uses a dictionnary.')\n",
    "    feature.add_argument('-g','--ngrams-range', metavar=('MIN_NGRAM','MAX_NGRAM'), nargs='*', default=[1,9], help='Range of ngrams to generate. ngrams in [minNgram,maxNgram[.')\n",
    "    feature.add_argument('-f','--num-features-range', metavar=('MIN_FATURES','MAX_FATURES'), nargs='*', default=[4,100], help='If specified, during training each phrase will have a random number of features in range [minFeatures,maxFeatures[. None if take all.')\n",
    "\n",
    "    # Embedding options\n",
    "    embedding = parser.add_argument_group('Embedding options')\n",
    "    embedding.add_argument('--no-hashembed', action='store_true', default=False, help='Uses the default embedding.')\n",
    "    embedding.add_argument('--append-weight', action='store_true', default=False, help='Whether to append the importance parameters.')\n",
    "    embedding.add_argument('-D','--dim', type=int, default=20, help='Dimension of word vectors. Higher improves downstream task for fixed vocabulary size.')\n",
    "    embedding.add_argument('-B','--num-buckets', type=int, default=10**6, help='Number of buckets in the shared embedding table. Higher improves approximation quality.')\n",
    "    embedding.add_argument('-N','--num-embeding', type=int, default=10**7, help='Number of rows in the importance matrix. Approximate the number of rows in a usual embedding. Higher will increase possible vocabulary size.')\n",
    "    embedding.add_argument('-H','--num-hash', type=int, default=2, help='Number of different hashes to use. Higher improves approximation quality.')\n",
    "\n",
    "    args = parser.parse_args(l)\n",
    "    args.plateau_reduce_lr = check_pair(parser,args.plateau_reduce_lr,\"plateau-reduce-lr\",types=(int,float))\n",
    "    args.ngrams_range = check_pair(parser,args.ngrams_range,\"ngrams-range\")\n",
    "    feature.num_features_range = check_pair(parser,args.num_features_range,\"num-features-range\")\n",
    "\n",
    "    return args\n",
    "\n",
    "def main(args):\n",
    "    \"\"\"Simply redirrcts to the correct function.\"\"\" \n",
    "    \n",
    "    args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "    print(vars(args))\n",
    "    return args\n",
    "    \n",
    "def run(s=\"\"):\n",
    "    args = parse_arguments(s.split())\n",
    "    return main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': 'ag', 'no_shuffle': False, 'no_checkpoint': False, 'epochs': 7, 'batch_size': 32, 'validation_size': 0.05, 'seed': 123, 'patience': 1, 'verbose': 3, 'plateau_reduce_lr': None, 'no_cuda': False, 'num_workers': 0, 'dictionnary': False, 'ngrams_range': [1, 4], 'num_features_range': [4, 100], 'no_hashembed': True, 'append_weight': False, 'dim': 20, 'num_buckets': 10000, 'num_embeding': 100000, 'num_hash': 2, 'cuda': False}\n"
     ]
    }
   ],
   "source": [
    "args = run('-N 100000 -B 10000 -f 4 100 -g 1 4 -b 32 -e 7 -p 1 -P None --no-hashembed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Ran on 2018-01-22 02:50\n",
      "\n",
      "Parameters: {'dataset': 'ag', 'no_shuffle': False, 'no_checkpoint': False, 'epochs': 7, 'batch_size': 32, 'validation_size': 0.05, 'seed': 123, 'patience': 1, 'verbose': 3, 'plateau_reduce_lr': None, 'no_cuda': False, 'num_workers': 0, 'dictionnary': False, 'ngrams_range': [1, 4], 'num_features_range': [4, 100], 'no_hashembed': True, 'append_weight': False, 'dim': 20, 'num_buckets': 10000, 'num_embeding': 100000, 'num_hash': 2, 'cuda': False}\n",
      "\n",
      "Prepares data ...\n",
      "Prepares model ...\n",
      "CPU times: user 2.37 s, sys: 37.5 ms, total: 2.41 s\n",
      "Wall time: 2.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "print(\"-------------------------------------------------\")\n",
    "print(\"Ran on {}\".format(time.strftime(\"%Y-%m-%d %H:%M\")))\n",
    "print()\n",
    "\n",
    "print('Parameters: {}'.format(vars(args)))\n",
    "print()\n",
    "\n",
    "# PREPARES DATA\n",
    "print('Prepares data ...')\n",
    "train, valid, test = train_valid_test_datasets(args.dataset,\n",
    "                                              validSize=args.validation_size,\n",
    "                                              isHashingTrick = not args.dictionnary,\n",
    "                                              nFeaturesRange = args.num_features_range,\n",
    "                                              ngramRange = args.ngrams_range,\n",
    "                                              seed = args.seed,\n",
    "                                              num_words = args.num_embeding,\n",
    "                                              specificArgs = {'dictionnary': ['num_words']})\n",
    "\n",
    "num_classes = len(train.classes)\n",
    "train = DataLoader(dataset=train, batch_size=args.batch_size, shuffle=not args.no_shuffle)\n",
    "valid = DataLoader(dataset=valid, batch_size=args.batch_size, shuffle=not args.no_shuffle)\n",
    "test = DataLoader(dataset=test, batch_size=args.batch_size, shuffle=not args.no_shuffle)\n",
    "\n",
    "# PREPARES MODEL\n",
    "print('Prepares model ...')\n",
    "model = ModelNoDict(args.num_embeding,\n",
    "                    args.dim,\n",
    "                    num_classes,\n",
    "                    isHash=not args.no_hashembed,\n",
    "                    num_buckets=args.num_buckets,\n",
    "                    append_weight=args.append_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ModuleTrainer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsample.modules import ModuleTrainer\n",
    "from torchsample.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from torchsample.initializers import XavierUniform\n",
    "from torchsample.metrics import CategoricalAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "callbacks = []\n",
    "if args.patience is not None:\n",
    "    callbacks.append(EarlyStopping(patience=args.patience))\n",
    "if args.plateau_reduce_lr is not None:\n",
    "    callbacks.append(ReduceLROnPlateau(factor=args.plateau_reduce_lr[1], patience=args.plateau_reduce_lr[0]))\n",
    "if not args.no_checkpoint:\n",
    "    callbacks.append(ModelCheckpoint('./', save_best_only=True, max_save=1))\n",
    "             \n",
    "#initializers = [XavierUniform(bias=False, module_filter='fc*')]\n",
    "metrics = [CategoricalAccuracy()]\n",
    "\n",
    "trainer.compile(loss=loss,\n",
    "                optimizer=optimizer,\n",
    "                callbacks=callbacks,\n",
    "                metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/7: : 298 batches [00:08, 36.57 batches/s, loss=0.665, val_loss=0.443, val_acc=83.20, acc=62.00]                 \n",
      "Epoch 2/7: : 298 batches [00:07, 37.64 batches/s, loss=0.384, val_loss=0.385, val_acc=85.40, acc=86.57]                 \n",
      "Epoch 3/7: : 298 batches [00:07, 38.97 batches/s, loss=0.218, val_loss=0.433, val_acc=86.00, acc=92.47]                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.2 s, sys: 2.76 s, total: 25 s\n",
      "Wall time: 23.7 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer.fit_loader(train,\n",
    "                   val_loader=valid,\n",
    "                   num_epoch=args.epochs,\n",
    "                   verbose=args.verbose,\n",
    "                   cuda_device=0 if args.cuda else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.4417393133044243, Accuracy: 86.4\n",
      "Test - Loss: 0.6335211604147776, Accuracy: 84.71052631578948\n"
     ]
    }
   ],
   "source": [
    "evalTest = trainer.evaluate_loader(test)\n",
    "evalValid = trainer.evaluate_loader(valid)\n",
    "print(\"Validation - Loss: {}, Accuracy: {}\".format(evalValid['val_loss'],evalValid['val_acc_metric']))\n",
    "print(\"Test - Loss: {}, Accuracy: {}\".format(evalTest['val_loss'],evalTest['val_acc_metric']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('ckpt.pth.tar')\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid {'val_loss': 0.37180668553885293, 'val_acc_metric': 87.2}\n",
      "test {'val_loss': 0.5179101362606224, 'val_acc_metric': 85.34210526315789}\n"
     ]
    }
   ],
   "source": [
    "print(\"valid\",trainer.evaluate_loader(valid))\n",
    "print(\"test\",trainer.evaluate_loader(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.853421052631579\n"
     ]
    }
   ],
   "source": [
    "trainer.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:47:36.128977Z",
     "start_time": "2018-01-18T15:46:52.310547Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.91 s, sys: 20.7 ms, total: 1.93 s\n",
      "Wall time: 1.93 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "train,valid,test = train_valid_test_datasets(args.dataset,\n",
    "                                              validSize=args.validation_size,\n",
    "                                              isHashingTrick = not args.dictionnary,\n",
    "                                              nFeaturesRange = args.num_features_range,\n",
    "                                              ngramRange = args.ngrams_range,\n",
    "                                              seed = args.seed,\n",
    "                                              num_words = args.num_embeding,\n",
    "                                              specificArgs = {'dictionnary': ['num_words']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:47:36.134974Z",
     "start_time": "2018-01-18T15:47:36.131757Z"
    }
   },
   "outputs": [],
   "source": [
    "#train, valid = train_valid_load(train,validSize=0.1,isShuffle=True,seed=123,batch_size=batchSize)\n",
    "test = DataLoader(dataset=test,batch_size=args.batch_size,shuffle=not args.no_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T15:47:40.949139Z",
     "start_time": "2018-01-18T15:47:36.141801Z"
    }
   },
   "outputs": [],
   "source": [
    "num_classes = len(train.classes)\n",
    "model = ModelNoDict(args.num_embeding,\n",
    "                    args.dim,\n",
    "                    num_classes,\n",
    "                    isHash=not args.no_hashembed,\n",
    "                    num_buckets=args.num_buckets)\n",
    "trainer = Trainer(model)\n",
    "callbacks = [EarlyStopping(patience=args.patience)] if args.patience is not None else [None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num parameters in model: 400092\n",
      "Train on 297 samples, validate on 16 samples\n",
      "Time since start: 0.1. Epoch: 0. Loss: 0.8240941166877747. Acc: 0.8.\n",
      "Time since start: 0.2. Epoch: 1. Loss: 0.47239628434181213. Acc: 0.856.\n",
      "Time since start: 0.4. Epoch: 2. Loss: 0.33095020055770874. Acc: 0.856.\n",
      "Time since start: 0.5. Epoch: 3. Loss: 0.2885614335536957. Acc: 0.872.\n",
      "Time since start: 0.6. Epoch: 4. Loss: 0.23835055530071259. Acc: 0.868.\n",
      "Time since start: 0.7. Epoch: 5. Loss: 0.20833487808704376. Acc: 0.866.\n",
      "Time since start: 0.9. Epoch: 6. Loss: 0.23387226462364197. Acc: 0.876.\n",
      "CPU times: user 48.4 s, sys: 3.03 s, total: 51.4 s\n",
      "Wall time: 51.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer(train,\n",
    "        validDataset = valid,\n",
    "        callbacks = callbacks,\n",
    "        batch_size = args.batch_size,\n",
    "        epochs = args.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.8522368421052632\n"
     ]
    }
   ],
   "source": [
    "trainer.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-18T15:46:57.185Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num parameters in model: 1200412\n",
      "Train on 891 samples, validate on 47 samples\n",
      "Epoch: 0. Loss: 0.6332538723945618. Acc: 0.904.\n",
      "Epoch: 1. Loss: 0.5259655714035034. Acc: 0.906.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Hash-Embeddings/evaluate/pipeline/trainer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, trainDataset, validDataset, validSize, callbacks, batch_size, epochs, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Hash-Embeddings/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Hash-Embeddings/evaluate/pipeline/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Hash-Embeddings/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Hash-Embeddings/evaluate/pipeline/embedding.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         shared_embedding = torch.stack([self.shared_embeddings(idx_shared_embeddings[:,:,iHash]) \n\u001b[0;32m--> 237\u001b[0;31m                                         for iHash in range(self.num_hashes)], dim=-1)\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0mimportance_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimportance_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_importance_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mimportance_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportance_weight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Hash-Embeddings/venv/lib/python3.6/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(sequence, dim, out)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer(train,\n",
    "        validDataset = valid,\n",
    "        callbacks = callbacks,\n",
    "        batch_size = args.batch_size,\n",
    "        epochs = args.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-18T15:46:59.095Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accurate: 0.9014473684210527\n"
     ]
    }
   ],
   "source": [
    "trainer.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsample.datasets import CSVDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sys' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-75f6475f3fca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mCSVDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Hash-Embeddings/venv/src/torchsample/torchsample/datasets.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, csv, input_cols, target_cols, input_transform, target_transform, co_transform)\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0mduring\u001b[0m \u001b[0mruntime\u001b[0m \u001b[0mloading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         \"\"\"\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0mcheck_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pandas\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_cols_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Hash-Embeddings/venv/src/torchsample/torchsample/datasets.py\u001b[0m in \u001b[0;36mcheck_module\u001b[0;34m(module)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcheck_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{} module not imported. Please use \"pip install {}\".'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sys' is not defined"
     ]
    }
   ],
   "source": [
    "CSVDataset('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
